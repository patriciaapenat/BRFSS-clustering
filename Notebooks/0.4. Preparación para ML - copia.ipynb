{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2dd9c5-4a01-4324-9652-15e8795414c3",
   "metadata": {},
   "source": [
    "<div style=\"position: absolute; top: 0; left: 0; font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <a href=\"https://github.com/patriciaapenat\" style=\"text-decoration: none; color: inherit;\">Patricia Peña Torres</a>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 48px;\">\n",
    "    <strong>Proyecto final, BRFSS-clustering</strong>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 36px;\">\n",
    "    <strong>0.2. Análisis exploratorio</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996665d-371b-49eb-aed3-c9bdc041a87a",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79c08c-36ef-4bf1-9466-3c2fe33aee97",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "En este notebook se llava a cabo lo relativo al análisis exploratorio, por la naturaleza de los datos este EDA se ha centrado principalmente en variables demográficas\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f5ac0-5401-4237-903b-5763e0ee97c6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <strong>Configuración del entorno de trabajo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b597558-4d8e-4ba1-9bfe-71a7482505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from scipy.spatial import KDTree\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from functools import reduce\n",
    "\n",
    "# Ignorar advertencias deprecated\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f6a3e-1bbc-4f71-b4be-5272a040457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar gráficos\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2995-edfa-4aeb-b366-b25220be6cbd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Configuración de Spark</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1edd3-f886-4c44-81bf-d6052444175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay un SparkContext existente, debemos cerrarlo antes de crear uno nuevo\n",
    "if 'sc' in locals() and sc:\n",
    "    sc.stop()  # Detener el SparkContext anterior si existe\n",
    "\n",
    "# Configuración de Spark\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .setMaster(\"local[2]\")  # Modo local con un hilo para ejecución\n",
    "    .set(\"spark.driver.host\", \"127.0.0.1\")  # Dirección del host del driver\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"3600s\")  # Intervalo de latido del executor\n",
    "    .set(\"spark.network.timeout\", \"7200s\")  # Tiempo de espera de la red\n",
    "    .set(\"spark.executor.memory\", \"14g\")  # Memoria asignada para cada executor\n",
    "    .set(\"spark.driver.memory\", \"14g\")  # Memoria asignada para el driver\n",
    ")\n",
    "\n",
    "# Crear un nuevo SparkContext con la configuración especificada\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Configuración de SparkSession (interfaz de alto nivel para trabajar con datos estructurados en Spark)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Habilitar la evaluación perezosa en Spark SQL REPL\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 1000)  # Número máximo de filas a mostrar en la evaluación perezosa\n",
    "    .getOrCreate()  # Obtener la sesión Spark existente o crear una nueva si no existe\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069114-4bba-425b-b2ff-5e109d12fbe3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Lectura del archivo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87dc7af-efec-4874-a40a-b6d60a0fb936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"C:\\\\Users\\\\patri\\\\OneDrive - UAB\\\\Documentos\\\\GitHub\\\\BRFSS-clustering\\\\datos\\\\BRFSS_imputated_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a900d-2deb-4c3f-b090-0a800fe1fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las columnas a tipo numérico\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumn(column_name, col(column_name).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914853c3-7ee8-4c2f-bb8f-48853efa11d2",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\r\n",
    "Un autoencoder es un tipo de red neuronal artificial utilizada en tareas de aprendizaje no supervisado, específicamente en tareas de reducción de dimensionalidad y generación de datos. Su estructura consta de dos partes principales: el codificador (encoder) y el decodificador (decoder). El objetivo principal de un autoencoder es aprender una representación comprimida de los datos de entrada y luego reconstruir los datos originales a partir de esta representación comprimida.\r\n",
    "\r\n",
    "Aquí hay una breve descripción de cada una de las partes de un autoencoder:\r\n",
    "\r\n",
    "1. Codificador (Encoder):\r\n",
    "   - La parte del codificador toma los datos de entrada y los transforma en una representación de menor dimensionalidad (también llamada \"código\" o \"embedding\").\r\n",
    "   - A medida que la red neuronal del codificador reduce la dimensionalidad, está aprendiendo a capturar las características más importantes y relevantes de los datos de entrada.\r\n",
    "   - El codificador puede consistir en una o varias capas ocultas, típicamente utilizando funciones de activación como ReLU (Rectified Linear Unit) en cada capa.\r\n",
    "\r\n",
    "2. Decodificador (Decoder):\r\n",
    "   - La parte del decodificador toma la representación comprimida del codificador y la expande nuevamente para reconstruir los datos originales.\r\n",
    "   - La red del decodificador es esencialmente un espejo inverso del codificador, donde las capas ocultas aumentan gradualmente la dimensionalidad de los datos.\r\n",
    "   - El decodificador utiliza una función de activación adecuada en la capa de salida para generar la reconstrucción.\r\n",
    "\r\n",
    "La idea clave detrás de un autoencoder es que la red intenta aprender una representación eficiente de los datos, de modo que la reconstrucción sea lo más cercana posible a los datos originales. El proceso de entrenamiento implica minimizar la diferencia entre los datos de entrada y los datos reconstruidos, lo que fomenta la captura de patrones significativos en los datos.\r\n",
    "\r\n",
    "Los autoencoders tienen diversas aplicaciones, como la reducción de ruido en imágenes, la detección de anomalías, la generación de imágenes sintéticas y la reducción de dimensionalidad para visualización y compresión de datos.\r\n",
    "\r\n",
    "En Keras, puedes implementar un autoencoder utilizando su API de alto nivel, que facilita la construcción y entrenamiento de redes neuronales. Puedes definir un modelo de autoencoder utilizando capas Dense (totalmente conectadas) para el codificador y el decodificador, y luego compilar y entrenar el modelo utilizando datos de entrada y objetivos de reconstrucción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd325f6-2741-4d3b-9e09-fb0aea9578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_features = [col for col in df.columns if col != \"etiqueta\"]\n",
    "ensamblador = VectorAssembler(inputCols=columnas_features, outputCol=\"features\")\n",
    "df_con_features = ensamblador.transform(df).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75e168-9eaf-48d1-aea2-d68d7d22b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "features_array = np.array(df_con_features.rdd.map(lambda x: x.features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad851d-bc73-47b6-8751-312ac4c180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el autoencoder utilizando TensorFlow\n",
    "input_dim = len(columnas_features)\n",
    "encoding_dim = 4  # Dimensión reducida deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb8daf-b6c0-4484-abb4-4c4b409eda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del autoencoder\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoder = tf.keras.layers.Dense(encoding_dim, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(input_layer)\n",
    "decoder = tf.keras.layers.Dense(input_dim, activation='relu')(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0d9f1-d55f-4f9b-ab35-76619b2c3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo del autoencoder\n",
    "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ca1a-679d-4321-8973-366427839b1c",
   "metadata": {},
   "source": [
    "Un autoencoder es un tipo de red neuronal que consta de dos partes principales: el codificador $(f(x))$ y el decodificador $(g(z))$. El objetivo principal de un autoencoder es aprender una representación eficiente y comprimida de los datos de entrada, de modo que se puedan reconstruir con la menor pérdida de información posible. Para lograr esto, se utiliza una función de pérdida que mide la diferencia entre los datos de entrada originales $(x)$ y los datos reconstruidos $(x')$.\n",
    "\n",
    "1. Codificador (Encoder):\n",
    "   - El codificador toma un vector de entrada $x$ y lo mapea a un vector de representación comprimida $z$ a través de una serie de transformaciones lineales y no lineales. En este caso, el codificador utiliza una activación 'sigmoid' y un término de regularización L2 con $l=0.01$, lo que significa que se aplica una función sigmoide a la salida del codificador y se agrega un término de regularización L2 en la función de pérdida para controlar el sobreajuste:\n",
    "   $$ z = f(x) $$\n",
    "\n",
    "2. Decodificador (Decoder):\n",
    "   - El decodificador toma la representación comprimida $z$ y lo mapea de nuevo al espacio de entrada $x'$ tratando de reconstruir $x$ lo más fielmente posible. En este caso, el decodificador utiliza una activación 'relu':\n",
    "   $$ x' = g(z) $$\n",
    "\n",
    "3. Función de Pérdida (Loss Function):\n",
    "   - Para entrenar el autoencoder, se utiliza la función de pérdida 'categorical_crossentropy', que se usa comúnmente para problemas de clasificación multiclase. En este contexto, se utiliza para medir la discrepancia entre las etiquetas asignadas y las salidas del decodificador. La pérdida 'categorical_crossentropy' se utiliza para evaluar qué tan bien se asignan las etiquetas a las representaciones codificadas:\n",
    "   $$  {categorical_crossentropy}(y, y') = -\\sum_{i} y_i \\log(y'_i) $$\n",
    "\n",
    "   - Donde $y$ son las etiquetas reales y $y'$ son las salidas del decodificador.\n",
    "\n",
    "4. Entrenamiento:\n",
    "   - Durante el entrenamiento, el autoencoder busca minimizar la función de pérdida 'categorical_crossentropy', ajustando los parámetros del codificador y el decodificador. Esto se logra utilizando un optimizador Adam con tasas de aprendizaje adaptativas. El término de regularización L2 en el codificador ayuda a controlar el sobreajuste durante el entrenamiento.\n",
    "\n",
    "El objetivo final es que, después del entrenamiento, el autoencoder aprenda a capturar las características más importantes y relevantes de los datos de entrada en la representación $z$, de modo que la reconstrucción $x'$ sea una versión fiel de $x$, y la pérdida 'categorical_crossentropy' sea mínima. Las representaciones codificadas obtenidas pueden ser útiles en tareas de clasificación o análisis de datos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784100b6-a802-41d8-bfe2-457e7e094bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febf16c-a500-4954-ad17-7acc4755e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el autoencoder\n",
    "autoencoder.fit(features_array, features_array, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de07d7-9e07-4d39-8ce2-191d6a599dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las representaciones codificadas de los datos\n",
    "encoded_features_model = tf.keras.models.Model(inputs=input_layer, outputs=encoder)\n",
    "encoded_features = encoded_features_model.predict(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83911f27-11f2-4091-a8e4-e226fb1442a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071f959-07dc-414b-95a3-18d52df856f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las representaciones codificadas de vuelta a un DataFrame de Spark\n",
    "encoded_features_rdd = spark.sparkContext.parallelize(encoded_features.tolist())\n",
    "encoded_features_df = encoded_features_rdd.map(lambda x: (Vectors.dense(x),)).toDF([\"encoded_features\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107ac0b3-aa12-4487-9c45-c6ab77c2657d",
   "metadata": {},
   "source": [
    "encoded_features = encoded_features_df.withColumnRenamed(\"encoded_features\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53666c2-8e1c-42b4-a6e5-ed38cb3ce295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el autoencoder\n",
    "history = autoencoder.fit(features_array, features_array, epochs=50, batch_size=64)\n",
    "\n",
    "# Imprimir métricas durante el entrenamiento\n",
    "print(\"Métricas durante el entrenamiento:\")\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dcb28-5c85-43b2-8f8c-7a985d11a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la pérdida durante el entrenamiento\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c554-d04c-47f9-9020-9f9d479d94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "encoded_features_array = np.array(encoded_features_df.select(\"encoded_features\").rdd.map(lambda x: x.encoded_features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9339f6-c3ee-4930-94a7-380119b229e4",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f93acc-6bcd-48ec-8c57-efc17d493d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as SKLearnKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb53d6-980d-47a3-8913-42bcaca11203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista vacía para almacenar las inercias\n",
    "cs = []\n",
    "\n",
    "# Probar diferentes valores de k (número de clusters)\n",
    "for i in range(1, 20):\n",
    "    kmeans = SKLearnKMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(encoded_features_array)  # Usar array Numpy de características codificadas\n",
    "\n",
    "    # Calcular la inercia y añadirla a la lista\n",
    "    cs.append(kmeans.inertia_)\n",
    "\n",
    "# Trazar la curva de la inercia en función del número de clusters\n",
    "plt.plot(range(1, 20), cs, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Número de Clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Criterio del Codo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea342a-9833-4b77-894c-52f78c75bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, es necesario asegurarse de que las características estén en formato de vector. \n",
    "assembler = VectorAssembler(inputCols=[\"encoded_features\"], outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(encoded_features_df)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")  # k es el número de componentes principales a calcular\n",
    "model = pca.fit(vectorized_df)\n",
    "result = model.transform(vectorized_df)\n",
    "\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab15744-daa5-45c7-97ea-f2494af21e9f",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "\r\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de agrupación (clustering) que se utiliza para identificar grupos de puntos en un conjunto de datos en función de su densidad. A diferencia de otros algoritmos de agrupación como K-Means, DBSCAN no asume que los grupos tienen una forma esférica y puede identificar grupos de diferentes tamaños y formas de manera más flexible.\r\n",
    "\r\n",
    "El algoritmo DBSCAN funciona de la siguiente manera:\r\n",
    "\r\n",
    "1. Selecciona un punto de inicio aleatorio que aún no haya sido visitado ni asignado a ningún grupo.\r\n",
    "2. Examina los puntos cercanos a este punto de inicio dentro de un radio especificado llamado \"radio epsilon\" (ε).\r\n",
    "3. Si hay suficientes puntos cercanos dentro de ε, se considera que estos puntos forman un grupo, y se les asigna un etiqueta común.\r\n",
    "4. El proceso se repite para todos los puntos dentro del grupo recién formado, y se siguen expandiendo los grupos hasta que no se puedan encontrar más puntos cercanos dentro de ε.\r\n",
    "5. Si un punto no puede ser alcanzado por ningún otro punto dentro de ε, se considera un punto \"ruido\" y no se asigna a ningconsiderados puntos ruido.\r\n",
    "\r\n",
    "DBSCAN es especialmente útil cuando se trabaja con datos en los que los grupos pueden tener formas y tamaños irregulares, y puede ser una alternativa efectiva a otros algoritmos de agrupación más tradicionales como K-Means.ún grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2391f1b-5ec7-4f89-809b-4e6fae9fed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar los datos para que tengan media 0 y desviación estándar 1\n",
    "scaler = StandardScaler()\n",
    "scaled_encoded_features = scaler.fit_transform(encoded_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd7a7f-4af0-40eb-90c2-5b7509c44d28",
   "metadata": {},
   "source": [
    "Primero se ajusta el `StandardScaler` a tus datos (`encoded_features_array`) para calcular la media $mu$ y la desviación estándar $sigma$ de cada característica. Luego, transforma los datos restando la media y dividiendo por la desviación estándar para cada característica, resultando en `scaled_encoded_features`, donde cada característica ahora está estandarizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ab8a4-d5c2-425e-a6be-f7e26831a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_rdd = spark.sparkContext.parallelize(scaled_encoded_features.tolist())\n",
    "\n",
    "# Convertir el RDD a un DataFrame de Spark\n",
    "df_scaled = scaled_features_rdd.map(lambda x: (Vectors.dense(x),)).toDF([\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9279c2-1210-45fa-a69c-601127f5f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(point_idx, points, eps):\n",
    "    neighbors = []\n",
    "    point = np.array(points[point_idx])  # Convierte el punto de referencia a un array de Numpy\n",
    "    for idx, other_point in enumerate(points):\n",
    "        other_point_array = np.array(other_point)  # Convierte el otro punto a un array de Numpy\n",
    "        if np.linalg.norm(point - other_point_array) < eps:\n",
    "            neighbors.append(idx)\n",
    "    return neighbors\n",
    "\n",
    "neighbors = find_neighbors(scaled_features_rdd, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603456b-b8c3-439a-8fbb-4a84ac38a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(point_idx, points, eps):\n",
    "    neighbors = []\n",
    "    for idx, other_point in enumerate(points):\n",
    "        if np.linalg.norm(points[point_idx] - other_point) < eps:\n",
    "            neighbors.append(idx)\n",
    "    return neighbors\n",
    "\n",
    "def expand_cluster(point_idx, neighbors, cluster_id, eps, min_points, points, clusters, visited):\n",
    "    clusters[point_idx] = cluster_id\n",
    "    i = 0\n",
    "    while i < len(neighbors):\n",
    "        neighbor_idx = neighbors[i]\n",
    "        if not visited[neighbor_idx]:\n",
    "            visited[neighbor_idx] = True\n",
    "            new_neighbors = find_neighbors(neighbor_idx, points, eps)\n",
    "            if len(new_neighbors) >= min_points:\n",
    "                neighbors = neighbors + new_neighbors\n",
    "        if clusters[neighbor_idx] == -1:\n",
    "            clusters[neighbor_idx] = cluster_id\n",
    "        i += 1\n",
    "\n",
    "def dbscan(points, eps, min_points):\n",
    "    cluster_id = 0\n",
    "    n_points = len(points)\n",
    "    clusters = [-1] * n_points\n",
    "    visited = [False] * n_points\n",
    "    \n",
    "    for point_idx in range(n_points):\n",
    "        if visited[point_idx]:\n",
    "            continue\n",
    "        visited[point_idx] = True\n",
    "        neighbors = find_neighbors(point_idx, points, eps)\n",
    "        if len(neighbors) < min_points:\n",
    "            clusters[point_idx] = -1  # Mark as noise\n",
    "        else:\n",
    "            expand_cluster(point_idx, neighbors, cluster_id, eps, min_points, points, clusters, visited)\n",
    "            cluster_id += 1\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0ebd2-b323-4f1b-8cdc-1648af3b020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'scaled_encoded_features' es un RDD\n",
    "def apply_dbscan_to_rdd(rdd, eps, min_samples):\n",
    "    # Esta función necesita ser adaptada para operar en el entorno distribuido de Spark\n",
    "    local_points = rdd.collect()  # Esto NO es recomendable en práctica por problemas de memoria\n",
    "    clusters = dbscan(local_points, eps, min_samples)\n",
    "    return clusters\n",
    "\n",
    "# Llamar a la función adaptada (esto es solo conceptual)\n",
    "cluster_labels = apply_dbscan_to_rdd(scaled_features_rdd, 5, 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea81f6-2b6c-4a9b-88cb-2f244d13abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "dbscan = DBSCAN(eps=5, min_samples=5)\n",
    "cluster_labels = dbscan.fit_predict(scaled_encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e2d4d-5053-497a-80c9-a758224a5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir las etiquetas de clúster de vuelta al DataFrame de Spark\n",
    "# Primero, necesitas convertir las etiquetas a un DataFrame de Spark\n",
    "labels_df = spark.createDataFrame(cluster_labels.tolist(), IntegerType())\n",
    "labels_df = labels_df.withColumn(\"row_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9a43f-0515-4fc9-9719-99e059273d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade un ID a tu DataFrame original para hacer join\n",
    "df = df.withColumn(\"row_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccae54-543c-4a01-8cc7-9d8acdb4c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir las etiquetas de clúster con el DataFrame original\n",
    "df_with_labels = df.join(labels_df, df.row_id == labels_df.row_id).drop(\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23188a-68cf-42ca-800a-e1664b1562a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_labels.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a65e21-b6ca-4a5c-b6a4-1a8390b27b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Calcula el Silhouette Score\n",
    "silhouette_score_value = silhouette_score(scaled_encoded_features, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_score_value)\n",
    "\n",
    "# Calcula el Coeficiente de Calinski-Harabasz\n",
    "calinski_harabasz_score_value = calinski_harabasz_score(scaled_encoded_features, cluster_labels)\n",
    "print(\"Calinski-Harabasz Score:\", calinski_harabasz_score_value)\n",
    "\n",
    "# Calcula el Davies-Bouldin Score\n",
    "davies_bouldin_score_value = davies_bouldin_score(scaled_encoded_features, cluster_labels)\n",
    "print(\"Davies-Bouldin Score:\", davies_bouldin_score_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f67c23-4bd3-4fde-bd42-46a8681d01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suponiendo que `encoded_features_array` contiene las representaciones codificadas de tus datos\n",
    "# y `cluster_labels` contiene las etiquetas de clúster asignadas por DBSCAN\n",
    "\n",
    "# Reducir la dimensionalidad con t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_encoded_features = tsne.fit_transform(encoded_features_array)\n",
    "\n",
    "# Crear un DataFrame de pandas para la visualización\n",
    "tsne_df = pd.DataFrame(tsne_encoded_features, columns=['Componente 1', 'Componente 2'])\n",
    "tsne_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualizar los clústeres en un gráfico de dispersión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Componente 1', y='Componente 2', hue='Cluster', data=tsne_df, palette='mako', legend='full')\n",
    "plt.title('Visualización de Clústeres con t-SNE')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7515a6-666d-4bfb-aa64-33f19da6c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que `encoded_features_array` contiene las representaciones codificadas de tus datos\n",
    "# y `cluster_labels` contiene las etiquetas de clúster asignadas por DBSCAN\n",
    "\n",
    "# Reducir la dimensionalidad con t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_encoded_features = tsne.fit_transform(encoded_features_array)\n",
    "\n",
    "# Crear un DataFrame de pandas para la visualización\n",
    "tsne_df = pd.DataFrame(tsne_encoded_features, columns=['Componente 1', 'Componente 2'])\n",
    "tsne_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Filtrar los datos para visualizar solo los cuatro clústeres\n",
    "tsne_df_filtered = tsne_df[tsne_df['Cluster'] >= 0]  # Solo considerar clústeres con etiquetas positivas\n",
    "\n",
    "# Visualizar los clústeres en un gráfico de dispersión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='Componente 1', y='Componente 2', hue='Cluster', data=tsne_df_filtered, palette='mako', legend='full')\n",
    "plt.title('Visualización de Clústeres con t-SNE')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914ca63-50dd-4945-89db-aecdf7a6c5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
