{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2dd9c5-4a01-4324-9652-15e8795414c3",
   "metadata": {},
   "source": [
    "<div style=\"position: absolute; top: 0; left: 0; font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <a href=\"https://github.com/patriciaapenat\" style=\"text-decoration: none; color: inherit;\">Patricia Peña Torres</a>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 48px;\">\n",
    "    <strong>Proyecto final, BRFSS-clustering</strong>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 36px;\">\n",
    "    <strong>0.2. Análisis exploratorio</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996665d-371b-49eb-aed3-c9bdc041a87a",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79c08c-36ef-4bf1-9466-3c2fe33aee97",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "En este notebook se llava a cabo lo relativo al análisis exploratorio, por la naturaleza de los datos este EDA se ha centrado principalmente en variables demográficas\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f5ac0-5401-4237-903b-5763e0ee97c6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <strong>Configuración del entorno de trabajo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b597558-4d8e-4ba1-9bfe-71a7482505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import Imputer\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import warnings\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ignorar advertencias deprecated\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17f6a3e-1bbc-4f71-b4be-5272a040457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar gráficos\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2995-edfa-4aeb-b366-b25220be6cbd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Configuración de Spark</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c1edd3-f886-4c44-81bf-d6052444175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay un SparkContext existente, debemos cerrarlo antes de crear uno nuevo\n",
    "if 'sc' in locals() and sc:\n",
    "    sc.stop()  # Detener el SparkContext anterior si existe\n",
    "\n",
    "# Configuración de Spark\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .setMaster(\"local[1]\")  # Modo local con un hilo para ejecución\n",
    "    .set(\"spark.driver.host\", \"127.0.0.1\")  # Dirección del host del driver\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"3600s\")  # Intervalo de latido del executor\n",
    "    .set(\"spark.network.timeout\", \"7200s\")  # Tiempo de espera de la red\n",
    "    .set(\"spark.executor.memory\", \"8g\")  # Memoria asignada para cada executor\n",
    "    .set(\"spark.driver.memory\", \"8g\")  # Memoria asignada para el driver\n",
    ")\n",
    "\n",
    "# Crear un nuevo SparkContext con la configuración especificada\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Configuración de SparkSession (interfaz de alto nivel para trabajar con datos estructurados en Spark)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Habilitar la evaluación perezosa en Spark SQL REPL\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 1000)  # Número máximo de filas a mostrar en la evaluación perezosa\n",
    "    .getOrCreate()  # Obtener la sesión Spark existente o crear una nueva si no existe\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069114-4bba-425b-b2ff-5e109d12fbe3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Lectura del archivo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec195683-bd8e-4a86-ad35-2f831c3edd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"C:\\Users\\patri\\OneDrive - UAB\\Documentos\\GitHub\\BRFSS-clustering\\datos\\BRFSS_Cleaner_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59f76a7-7589-4863-8de0-1211cc99c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del DataFrame: Filas=363130, Columnas=211\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las dimensiones del DataFrame\n",
    "print(f\"Dimensiones del DataFrame: Filas={df.count()}, Columnas={len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b61d1-00cc-489b-975a-2b258f1cc21d",
   "metadata": {},
   "source": [
    "\r\n",
    "# Imputación de valores nulos\r\n",
    "\r\n",
    "Es crucial abordar la presencia de valores nulos en nuestros datos, ya que estos podrían generar errores al intentar entrenar algoritmos. Ante esta situación, contamos con dos opciones principales:\r\n",
    "\r\n",
    "1. **Imputación Estadística:**\r\n",
    "   Podemos optar por realizar una imputación estadística, donde rellenamos los valores nulos con estadísticas descriptivas, como la media. Esta técnica es útil cuando buscamos proporcionar valores representativos para los datos faltantes.\r\n",
    "\r\n",
    "2. **Relleno con Valor Específico:**\r\n",
    "   Otra opción es utilizar el método `fillna` para rellenar los valores nulos con un valor específico, como 0. En este contexto, asignamos el valor 0 para indicar que cierta información \"no aplica\".\r\n",
    "\r\n",
    "En nuestra estrategia, hemos aplicado métodos diferentes según la naturaleza de las variables. Algunas variables, relacionadas con preguntas que podrían no aplicar a todos los casos, se imputaron con el método de relleno con un valor específico. Por otro lado, para aquellas variables donde los valores simplemente no estaban disponibles, optamos por la imputación estadística utilizando la media, buscando proporcionar una estimación razonable para completar los datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8222a7-1dd9-4ed3-86aa-1320ae435b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las columnas a tipo numérico\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumn(column_name, col(column_name).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba8c3006-6e51-4506-beb3-cf2fbcae33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el imputador\n",
    "imputer = Imputer(\n",
    "    inputCols=df.columns,\n",
    "    outputCols=[f\"{col}_imputed\" for col in df.columns],\n",
    "    missingValue=float('nan'),\n",
    "    strategy='mean'\n",
    ")\n",
    "\n",
    "# Ajustar y transformar el DataFrame\n",
    "model = imputer.fit(df)\n",
    "df_imputed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe1201a-bcb7-478e-9b02-3b87ed74537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col_name for col_name in df_imputed.columns if \"_imputed\" not in col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3fb57b-6a01-4bea-b2b3-e9f926bae750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed2 = df_imputed.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab740ed-35bf-4dd8-8a6f-fd61a557b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame como CSV\n",
    "df_imputed2.write.csv(\"C:\\\\Users\\\\patri\\\\OneDrive - UAB\\\\Documentos\\\\GitHub\\\\BRFSS-clustering\\\\datos\\\\BRFSS_imputated_2022.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4037a8bd-f246-45d7-b8dd-bc5804705fca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columnas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Iterar sobre todas las columnas sin limitación\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mcolumnas\u001b[49m), tamaño_grupo):\n\u001b[0;32m      3\u001b[0m     grupo_columnas \u001b[38;5;241m=\u001b[39m columnas[i:i \u001b[38;5;241m+\u001b[39m tamaño_grupo]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Seleccionar solo las columnas del grupo actual\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'columnas' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterar sobre todas las columnas sin limitación\n",
    "for i in range(0, len(columnas), tamaño_grupo):\n",
    "    grupo_columnas = columnas[i:i + tamaño_grupo]\n",
    "\n",
    "    # Seleccionar solo las columnas del grupo actual\n",
    "    df_grupo = df_imputed2.select(grupo_columnas)\n",
    "\n",
    "    # Filtrar los valores nulos en el DataFrame\n",
    "    df_grupo = df_grupo.dropna()\n",
    "\n",
    "    # Verificar si el DataFrame es vacío después de filtrar los nulos\n",
    "    if df_grupo.count() == 0:\n",
    "        print(f\"El DataFrame df_grupo está vacío después de filtrar los nulos para las columnas: {grupo_columnas}\")\n",
    "        continue\n",
    "\n",
    "    # Resto del código para la creación del boxplot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.boxplot(data=df_grupo.toPandas(), palette='mako')\n",
    "    plt.title(f'Boxplots para las columnas: {\", \".join(grupo_columnas)}')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Guardar el gráfico en la ruta especificada\n",
    "    nombre_archivo = f\"boxplot_grupo_{i + 1}_{min(i + tamaño_grupo, len(columnas))}.png\"\n",
    "    ruta_completa = ruta_guardado + nombre_archivo\n",
    "    plt.savefig(ruta_completa)\n",
    "\n",
    "    # Cerrar la figura para liberar memoria\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
