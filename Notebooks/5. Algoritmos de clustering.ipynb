{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2dd9c5-4a01-4324-9652-15e8795414c3",
   "metadata": {},
   "source": [
    "<div style=\"position: absolute; top: 0; left: 0; font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <a href=\"https://github.com/patriciaapenat\" style=\"text-decoration: none; color: inherit;\">Patricia Peña Torres</a>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 48px;\">\n",
    "    <strong>Proyecto final, BRFSS-clustering</strong>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 36px;\">\n",
    "    <strong>5. Aplicación de algoritmos de clustering</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996665d-371b-49eb-aed3-c9bdc041a87a",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79c08c-36ef-4bf1-9466-3c2fe33aee97",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "En este notebook se llava a cabo lo relativo al análisis exploratorio, por la naturaleza de los datos este EDA se ha centrado principalmente en variables demográficas\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f5ac0-5401-4237-903b-5763e0ee97c6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <strong>Configuración del entorno de trabajo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b597558-4d8e-4ba1-9bfe-71a7482505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from scipy.spatial import KDTree\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import seaborn as sns\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "from sklearn.cluster import KMeans as SKLearnKMeans\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# Ignorar advertencias deprecated\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17f6a3e-1bbc-4f71-b4be-5272a040457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar gráficos\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2995-edfa-4aeb-b366-b25220be6cbd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Configuración de Spark</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c1edd3-f886-4c44-81bf-d6052444175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay un SparkContext existente, debemos cerrarlo antes de crear uno nuevo\n",
    "if 'sc' in locals() and sc:\n",
    "    sc.stop()  # Detener el SparkContext anterior si existe\n",
    "\n",
    "# Configuración de Spark\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .setMaster(\"local[2]\")  # Modo local con un hilo para ejecución\n",
    "    .set(\"spark.driver.host\", \"127.0.0.1\")  # Dirección del host del driver\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"3600s\")  # Intervalo de latido del executor\n",
    "    .set(\"spark.network.timeout\", \"7200s\")  # Tiempo de espera de la red\n",
    "    .set(\"spark.executor.memory\", \"14g\")  # Memoria asignada para cada executor\n",
    "    .set(\"spark.driver.memory\", \"14g\")  # Memoria asignada para el driver\n",
    ")\n",
    "\n",
    "# Crear un nuevo SparkContext con la configuración especificada\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Configuración de SparkSession (interfaz de alto nivel para trabajar con datos estructurados en Spark)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Habilitar la evaluación perezosa en Spark SQL REPL\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 1000)  # Número máximo de filas a mostrar en la evaluación perezosa\n",
    "    .getOrCreate()  # Obtener la sesión Spark existente o crear una nueva si no existe\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069114-4bba-425b-b2ff-5e109d12fbe3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    <strong>Lectura del archivo</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87dc7af-efec-4874-a40a-b6d60a0fb936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"C:\\\\Users\\\\patri\\\\OneDrive - UAB\\\\Documentos\\\\GitHub\\\\BRFSS-clustering\\\\datos\\\\BRFSS_imputated_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7a900d-2deb-4c3f-b090-0a800fe1fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las columnas a tipo numérico\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumn(column_name, col(column_name).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914853c3-7ee8-4c2f-bb8f-48853efa11d2",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\r\n",
    "Un autoencoder es un tipo de red neuronal artificial utilizada en tareas de aprendizaje no supervisado, específicamente en tareas de reducción de dimensionalidad y generación de datos. Su estructura consta de dos partes principales: el codificador (encoder) y el decodificador (decoder). El objetivo principal de un autoencoder es aprender una representación comprimida de los datos de entrada y luego reconstruir los datos originales a partir de esta representación comprimida.\r\n",
    "\r\n",
    "Aquí hay una breve descripción de cada una de las partes de un autoencoder:\r\n",
    "\r\n",
    "1. Codificador (Encoder):\r\n",
    "   - La parte del codificador toma los datos de entrada y los transforma en una representación de menor dimensionalidad (también llamada \"código\" o \"embedding\").\r\n",
    "   - A medida que la red neuronal del codificador reduce la dimensionalidad, está aprendiendo a capturar las características más importantes y relevantes de los datos de entrada.\r\n",
    "   - El codificador puede consistir en una o varias capas ocultas, típicamente utilizando funciones de activación como ReLU (Rectified Linear Unit) en cada capa.\r\n",
    "\r\n",
    "2. Decodificador (Decoder):\r\n",
    "   - La parte del decodificador toma la representación comprimida del codificador y la expande nuevamente para reconstruir los datos originales.\r\n",
    "   - La red del decodificador es esencialmente un espejo inverso del codificador, donde las capas ocultas aumentan gradualmente la dimensionalidad de los datos.\r\n",
    "   - El decodificador utiliza una función de activación adecuada en la capa de salida para generar la reconstrucción.\r\n",
    "\r\n",
    "La idea clave detrás de un autoencoder es que la red intenta aprender una representación eficiente de los datos, de modo que la reconstrucción sea lo más cercana posible a los datos originales. El proceso de entrenamiento implica minimizar la diferencia entre los datos de entrada y los datos reconstruidos, lo que fomenta la captura de patrones significativos en los datos.\r\n",
    "\r\n",
    "Los autoencoders tienen diversas aplicaciones, como la reducción de ruido en imágenes, la detección de anomalías, la generación de imágenes sintéticas y la reducción de dimensionalidad para visualización y compresión de datos.\r\n",
    "\r\n",
    "En Keras, puedes implementar un autoencoder utilizando su API de alto nivel, que facilita la construcción y entrenamiento de redes neuronales. Puedes definir un modelo de autoencoder utilizando capas Dense (totalmente conectadas) para el codificador y el decodificador, y luego compilar y entrenar el modelo utilizando datos de entrada y objetivos de reconstrucción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd325f6-2741-4d3b-9e09-fb0aea9578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_features = [col for col in df.columns if col != \"etiqueta\"]\n",
    "ensamblador = VectorAssembler(inputCols=columnas_features, outputCol=\"features\")\n",
    "df_con_features = ensamblador.transform(df).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a75e168-9eaf-48d1-aea2-d68d7d22b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "features_array = np.array(df_con_features.rdd.map(lambda x: x.features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ad851d-bc73-47b6-8751-312ac4c180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el autoencoder utilizando TensorFlow\n",
    "input_dim = len(columnas_features)\n",
    "encoding_dim = 4  # Dimensión reducida deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcb8daf-b6c0-4484-abb4-4c4b409eda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del autoencoder\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoder = tf.keras.layers.Dense(encoding_dim, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(input_layer)\n",
    "decoder = tf.keras.layers.Dense(input_dim, activation='relu')(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af0d9f1-d55f-4f9b-ab35-76619b2c3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo del autoencoder\n",
    "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ca1a-679d-4321-8973-366427839b1c",
   "metadata": {},
   "source": [
    "Un autoencoder es un tipo de red neuronal que consta de dos partes principales: el codificador $(f(x))$ y el decodificador $(g(z))$. El objetivo principal de un autoencoder es aprender una representación eficiente y comprimida de los datos de entrada, de modo que se puedan reconstruir con la menor pérdida de información posible. Para lograr esto, se utiliza una función de pérdida que mide la diferencia entre los datos de entrada originales $(x)$ y los datos reconstruidos $(x')$.\n",
    "\n",
    "1. Codificador (Encoder):\n",
    "   - El codificador toma un vector de entrada $x$ y lo mapea a un vector de representación comprimida $z$ a través de una serie de transformaciones lineales y no lineales. En este caso, el codificador utiliza una activación 'sigmoid' y un término de regularización L2 con $l=0.01$, lo que significa que se aplica una función sigmoide a la salida del codificador y se agrega un término de regularización L2 en la función de pérdida para controlar el sobreajuste:\n",
    "   $$ z = f(x) $$\n",
    "\n",
    "2. Decodificador (Decoder):\n",
    "   - El decodificador toma la representación comprimida $z$ y lo mapea de nuevo al espacio de entrada $x'$ tratando de reconstruir $x$ lo más fielmente posible. En este caso, el decodificador utiliza una activación 'relu':\n",
    "   $$ x' = g(z) $$\n",
    "\n",
    "3. Función de Pérdida (Loss Function):\n",
    "   - Para entrenar el autoencoder, se utiliza la función de pérdida 'categorical_crossentropy', que se usa comúnmente para problemas de clasificación multiclase. En este contexto, se utiliza para medir la discrepancia entre las etiquetas asignadas y las salidas del decodificador. La pérdida 'categorical_crossentropy' se utiliza para evaluar qué tan bien se asignan las etiquetas a las representaciones codificadas:\n",
    "   $$  {categorical_crossentropy}(y, y') = -\\sum_{i} y_i \\log(y'_i) $$\n",
    "\n",
    "   - Donde $y$ son las etiquetas reales y $y'$ son las salidas del decodificador.\n",
    "\n",
    "4. Entrenamiento: **pendiente cambiar**\n",
    "   - Durante el entrenamiento, el autoencoder busca minimizar la función de pérdida 'categorical_crossentropy', ajustando los parámetros del codificador y el decodificador. Esto se logra utilizando un optimizador Adam con tasas de aprendizaje adaptativas. El término de regularización L2 en el codificador ayuda a controlar el sobreajuste durante el entrenamiento.\n",
    "\n",
    "El objetivo final es que, después del entrenamiento, el autoencoder aprenda a capturar las características más importantes y relevantes de los datos de entrada en la representación $z$, de modo que la reconstrucción $x'$ sea una versión fiel de $x$, y la pérdida 'categorical_crossentropy' sea mínima. Las representaciones codificadas obtenidas pueden ser útiles en tareas de clasificación o análisis de datos posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784100b6-a802-41d8-bfe2-457e7e094bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febf16c-a500-4954-ad17-7acc4755e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 38959.8320\n",
      "Epoch 2/100\n",
      "11348/11348 [==============================] - 35s 3ms/step - loss: 26688.6465\n",
      "Epoch 3/100\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 17848.7031\n",
      "Epoch 4/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 11697.5020\n",
      "Epoch 5/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 7585.4727\n",
      "Epoch 6/100\n",
      "11348/11348 [==============================] - 37s 3ms/step - loss: 5003.9697\n",
      "Epoch 7/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 3556.0151\n",
      "Epoch 8/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 4308.1860\n",
      "Epoch 9/100\n",
      "11348/11348 [==============================] - 35s 3ms/step - loss: 3837.8423\n",
      "Epoch 10/100\n",
      "11348/11348 [==============================] - 39s 3ms/step - loss: 7018.6738\n",
      "Epoch 11/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 6260.3735\n",
      "Epoch 12/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 5492.8149\n",
      "Epoch 13/100\n",
      "11348/11348 [==============================] - 37s 3ms/step - loss: 6784.0122\n",
      "Epoch 14/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 6182.4341\n",
      "Epoch 15/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 5629.7280\n",
      "Epoch 16/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 5181.5229\n",
      "Epoch 17/100\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 4815.0210\n",
      "Epoch 18/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 4513.4619\n",
      "Epoch 19/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 4265.2671\n",
      "Epoch 20/100\n",
      "11348/11348 [==============================] - 38s 3ms/step - loss: 4060.7351\n",
      "Epoch 21/100\n",
      "11348/11348 [==============================] - 35s 3ms/step - loss: 3890.6860\n",
      "Epoch 22/100\n",
      "11348/11348 [==============================] - 31s 3ms/step - loss: 3749.7913\n",
      "Epoch 23/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 3632.2200\n",
      "Epoch 24/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 3534.0908\n",
      "Epoch 25/100\n",
      "11348/11348 [==============================] - 30s 3ms/step - loss: 3451.5659\n",
      "Epoch 26/100\n",
      "11348/11348 [==============================] - 30s 3ms/step - loss: 3382.3406\n",
      "Epoch 27/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 3323.6602\n",
      "Epoch 28/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 3273.9775\n",
      "Epoch 29/100\n",
      "11348/11348 [==============================] - 30s 3ms/step - loss: 3231.8508\n",
      "Epoch 30/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 3195.9341\n",
      "Epoch 31/100\n",
      "11348/11348 [==============================] - 38s 3ms/step - loss: 3165.2356\n",
      "Epoch 32/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 3138.8389\n",
      "Epoch 33/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 3116.0793\n",
      "Epoch 34/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 3013.6670\n",
      "Epoch 35/100\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 2822.8042\n",
      "Epoch 36/100\n",
      "11348/11348 [==============================] - 32s 3ms/step - loss: 2798.9431\n",
      "Epoch 37/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 2777.8438\n",
      "Epoch 38/100\n",
      "11348/11348 [==============================] - 39s 3ms/step - loss: 2761.3005\n",
      "Epoch 39/100\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 2746.2488\n",
      "Epoch 40/100\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 2731.4041\n",
      "Epoch 41/100\n",
      " 8736/11348 [======================>.......] - ETA: 7s - loss: 2714.2156"
     ]
    }
   ],
   "source": [
    "# Entrenar el autoencoder\n",
    "autoencoder.fit(features_array, features_array, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de07d7-9e07-4d39-8ce2-191d6a599dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las representaciones codificadas de los datos\n",
    "encoded_features_model = tf.keras.models.Model(inputs=input_layer, outputs=encoder)\n",
    "encoded_features = encoded_features_model.predict(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071f959-07dc-414b-95a3-18d52df856f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las representaciones codificadas de vuelta a un DataFrame de Spark\n",
    "encoded_features_rdd = spark.sparkContext.parallelize(encoded_features.tolist())\n",
    "encoded_features_df = encoded_features_rdd.map(lambda x: (Vectors.dense(x),)).toDF([\"encoded_features\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "107ac0b3-aa12-4487-9c45-c6ab77c2657d",
   "metadata": {},
   "source": [
    "encoded_features = encoded_features_df.withColumnRenamed(\"encoded_features\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53666c2-8e1c-42b4-a6e5-ed38cb3ce295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el autoencoder\n",
    "history = autoencoder.fit(features_array, features_array, epochs=100, batch_size=64)\n",
    "\n",
    "# Imprimir métricas durante el entrenamiento\n",
    "print(\"Métricas durante el entrenamiento:\")\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dcb28-5c85-43b2-8f8c-7a985d11a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la pérdida durante el entrenamiento\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c554-d04c-47f9-9020-9f9d479d94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "encoded_features_array = np.array(encoded_features_df.select(\"encoded_features\").rdd.map(lambda x: x.encoded_features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9339f6-c3ee-4930-94a7-380119b229e4",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3d9e6-e0e0-4244-b2c0-1cd49f9b09ff",
   "metadata": {},
   "source": [
    "Qué es Kmeans\n",
    "Explicación matemática\n",
    "y del código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb53d6-980d-47a3-8913-42bcaca11203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista vacía para almacenar las inercias\n",
    "cs = []\n",
    "\n",
    "# Probar diferentes valores de k (número de clusters)\n",
    "for i in range(1, 20):\n",
    "    kmeans = SKLearnKMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(encoded_features_array)  # Usar array Numpy de características codificadas\n",
    "\n",
    "    # Calcular la inercia y añadirla a la lista\n",
    "    cs.append(kmeans.inertia_)\n",
    "\n",
    "# Trazar la curva de la inercia en función del número de clusters\n",
    "plt.plot(range(1, 20), cs, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Número de Clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Criterio del Codo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdf45fcf-2fc5-4ad4-9df7-5050b638f9e5",
   "metadata": {},
   "source": [
    "# Primero, es necesario asegurarse de que las características estén en formato de vector. \n",
    "assembler = VectorAssembler(inputCols=[\"encoded_features\"], outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(encoded_features_df)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")  # k es el número de componentes principales a calcular\n",
    "model = pca.fit(vectorized_df)\n",
    "result = model.transform(vectorized_df)\n",
    "\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d3d7a-08fd-4f41-a771-4589a60fbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las características estén en formato de vector\n",
    "assembler = VectorAssembler(inputCols=[\"encoded_features\"], outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(encoded_features_df)\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(vectorized_df)\n",
    "scaled_df = scalerModel.transform(vectorized_df)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = pca.fit(scaled_df)\n",
    "result = pcaModel.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c7c92-a985-48c8-a1e6-7aee1db53eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el objeto KMeans\n",
    "kmeans = KMeans(k=2, featuresCol=\"pcaFeatures\", predictionCol=\"cluster\")\n",
    "\n",
    "# Entrenar el modelo KMeans\n",
    "kmeans_model = kmeans.fit(result)\n",
    "\n",
    "# Obtener las predicciones\n",
    "predictions = kmeans_model.transform(result)\n",
    "\n",
    "# Mostrar los resultados\n",
    "predictions.select('pcaFeatures', 'cluster').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb15457-d7ba-473a-90a5-64172fd09715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de silueta\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Coeficiente de silueta:\", silhouette)\n",
    "\n",
    "# Puntuación de Calinski-Harabasz\n",
    "calinski_harabasz = evaluator.evaluate(predictions, {evaluator.metricName: \"silhouette\"})\n",
    "print(\"Puntuación de Calinski-Harabasz:\", calinski_harabasz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d615b90-c41f-44ac-9d48-10f064ae968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir Spark DataFrame a Pandas DataFrame para visualización\n",
    "predictions_pd = predictions.toPandas()\n",
    "\n",
    "# Extraer componentes de 'pcaFeatures' en nuevas columnas\n",
    "predictions_pd['pca_x'] = pcaFeatures_array[:, 0]\n",
    "predictions_pd['pca_y'] = pcaFeatures_array[:, 1]\n",
    "\n",
    "# Visualización 2D de los clusters utilizando Seaborn y la paleta \"mako\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=predictions_pd, x='pca_x', y='pca_y', hue='cluster', palette='mako', s=50, legend='full')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('2D Visualization of Clusters with Mako Palette')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
