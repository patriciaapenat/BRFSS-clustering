{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2dd9c5-4a01-4324-9652-15e8795414c3",
   "metadata": {},
   "source": [
    "<div style=\"position: absolute; top: 0; left: 0; font-family: 'Garamond'; font-size: 16px;\">\n",
    "    <a href=\"https://github.com/patriciaapenat\" style=\"text-decoration: none; color: inherit;\">Patricia Peña Torres</a>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 48px;\">\n",
    "    <strong>Proyecto final, BRFSS-clustering</strong>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\" style=\"font-family: 'Garamond'; font-size: 36px;\">\n",
    "    <strong>5. Aplicación de algoritmos de clustering</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9996665d-371b-49eb-aed3-c9bdc041a87a",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79c08c-36ef-4bf1-9466-3c2fe33aee97",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "El siguiente cuaderno de Jupyter está diseñado para llevar a cabo un análisis de agrupamiento de datos utilizando PySpark y TensorFlow. En él, se configura el entorno, se carga y prepara un conjunto de datos, se entrena un autoencoder para aprender representaciones eficientes de los datos, se aplica el algoritmo K-Means para realizar el clustering en las representaciones codificadas, y finalmente, se evalúan y visualizan los resultados del clustering. Este cuaderno sirve como ejemplo de cómo combinar diversas herramientas y técnicas para realizar un análisis completo de agrupamiento de datos en un entorno de programación interactivo.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f5ac0-5401-4237-903b-5763e0ee97c6",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "## Configuración del entorno de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b597558-4d8e-4ba1-9bfe-71a7482505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from scipy.spatial import KDTree\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import seaborn as sns\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import PCA\n",
    "from sklearn.cluster import KMeans as SKLearnKMeans\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "# Ignorar advertencias deprecated\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17f6a3e-1bbc-4f71-b4be-5272a040457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurar gráficos\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", palette=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f2995-edfa-4aeb-b366-b25220be6cbd",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "### Configuración de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c1edd3-f886-4c44-81bf-d6052444175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay un SparkContext existente, debemos cerrarlo antes de crear uno nuevo\n",
    "if 'sc' in locals() and sc:\n",
    "    sc.stop()  # Detener el SparkContext anterior si existe\n",
    "\n",
    "# Configuración de Spark\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .setMaster(\"local[2]\")  # Modo local con un hilo para ejecución\n",
    "    .set(\"spark.driver.host\", \"127.0.0.1\")  # Dirección del host del driver\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"3600s\")  # Intervalo de latido del executor\n",
    "    .set(\"spark.network.timeout\", \"7200s\")  # Tiempo de espera de la red\n",
    "    .set(\"spark.executor.memory\", \"14g\")  # Memoria asignada para cada executor\n",
    "    .set(\"spark.driver.memory\", \"14g\")  # Memoria asignada para el driver\n",
    ")\n",
    "\n",
    "# Crear un nuevo SparkContext con la configuración especificada\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Configuración de SparkSession (interfaz de alto nivel para trabajar con datos estructurados en Spark)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Proyecto_PatriciaA_Peña\")  # Nombre de la aplicación en Spark\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)  # Habilitar la evaluación perezosa en Spark SQL REPL\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 1000)  # Número máximo de filas a mostrar en la evaluación perezosa\n",
    "    .getOrCreate()  # Obtener la sesión Spark existente o crear una nueva si no existe\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069114-4bba-425b-b2ff-5e109d12fbe3",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "### Lectura del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87dc7af-efec-4874-a40a-b6d60a0fb936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(r\"C:\\\\Users\\\\patri\\\\OneDrive - UAB\\\\Documentos\\\\GitHub\\\\BRFSS-clustering\\\\datos\\\\BRFSS_imputated_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7a900d-2deb-4c3f-b090-0a800fe1fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las columnas a tipo numérico\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumn(column_name, col(column_name).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914853c3-7ee8-4c2f-bb8f-48853efa11d2",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "# Autoencoder\n",
    "\n",
    "\r\n",
    "El autoencoder es una red neuronal artificial que se utiliza en tareas de aprendizaje no supervisado, y su función principal es aprender una representación comprimida de los datos de entrada. Esta representación comprimida, a menudo llamada \"código\" o \"embedding\", captura las características más importantes y relevantes de los datos originales. En el contexto de K-Means, el autoencoder se utiliza como un paso previo para mejorar la calidad de la representación de los datos antes de aplicar K-Means para la asignación de etiquetas a los clusters.\r\n",
    "\r\n",
    "El proceso general implica dos etapas:\r\n",
    "\r\n",
    "1. **Entrenamiento del Autoencoder**: Durante esta etapa, el autoencoder se entrena para aprender a comprimir y luego reconstruir los datos originales. El objetivo es minimizar la diferencia entre los datos de entrada y los datos reconstruidos, lo que lleva a la captura de patrones significativos en los datos y a una representación más eficiente. El resultado es una representación comprimida de los datos en un espacio de menor dimensionalidad.\r\n",
    "\r\n",
    "2. **Aplicación de K-Means en la Representación Comprimida**: Una vez que se ha entrenado el autoencoder, la representación comprimida se utiliza como entrada para el algoritmo K-Means. K-Means agrupará los datos en clusters en función de esta representación, lo que puede ser más efectivo que aplicar K-Means directamente a los datos originales de alta dimensionalidad. Los clusters resultantes se utilizan como etiquetas para asignar las instancias de datos a grupos similares.\r\n",
    "\r\n",
    "Este enfoque de preprocesamiento con un autoencoder puede ser beneficioso cuando se trabaja con datos de alta dimensionalidad, ruidosos o con características irrelevantes. Al reducir la dimensionalidad y mejorar la representación de los datos, se espera que K-Means produzca agrupamientos más significativos y efectivos en el espacio de la representación comprimida, lo que facilita la asignación de etiquetas y la interpretación de los resultados. de reconstrucción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd325f6-2741-4d3b-9e09-fb0aea9578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_features = [col for col in df.columns if col != \"etiqueta\"]\n",
    "ensamblador = VectorAssembler(inputCols=columnas_features, outputCol=\"features\")\n",
    "df_con_features = ensamblador.transform(df).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a75e168-9eaf-48d1-aea2-d68d7d22b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "features_array = np.array(df_con_features.rdd.map(lambda x: x.features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ad851d-bc73-47b6-8751-312ac4c180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el autoencoder utilizando TensorFlow\n",
    "input_dim = len(columnas_features)\n",
    "encoding_dim = 4  # Dimensión reducida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcb8daf-b6c0-4484-abb4-4c4b409eda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la arquitectura del autoencoder\n",
    "input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "encoder = tf.keras.layers.Dense(encoding_dim, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(input_layer)\n",
    "decoder = tf.keras.layers.Dense(input_dim, activation='relu')(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af0d9f1-d55f-4f9b-ab35-76619b2c3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo del autoencoder\n",
    "autoencoder = tf.keras.models.Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784100b6-a802-41d8-bfe2-457e7e094bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ca1a-679d-4321-8973-366427839b1c",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "\n",
    "El autoencoder consta de dos partes clave: el codificador $(f(x))$ y el decodificador $(g(z))$. Su objetivo principal es aprender una representación eficiente y comprimida de los datos de entrada, logrando esto mediante un proceso de entrenamiento que minimiza el error cuadrático medio (MSE) entre los datos originales $(x)$ y los datos reconstruidos $(x')$.\r\n",
    "\r\n",
    "El codificador toma un vector de entrada $x$ y lo transforma en una representación comprimida $z$ mediante transformaciones lineales y no lineales. El decodificador, a su vez, toma esta representación comprimida $z$ y la reconstruye nuevamente en $x'$. Durante el entrenamiento, se utiliza el MSE para medir la discrepancia cuadrática entre los datos reales y los datos reconstruidos.\r\n",
    "\r\n",
    "El proceso de entrenamiento tiene como objetivo ajustar los parámetros del codificador y el decodificador para minimizar el MSE. Esto se logra mediante el uso de un optimizador, como Adam con tasas de aprendizaje adaptativas. Además, en algunos casos se puede aplicar regularización L2 en el codificador para controlar el posible sobreajuste, aunque esta decisión depende de la configuración específica.\r\n",
    "\r\n",
    "En resumen, el autoencoder aprende a capturar las características más relevantes de los datos de entrada en la representación $z$, permitiendo una reconstrucción precisa de $x$ al minimizar el error cuadrático medio (MSE). Estas representaciones codificadas pueden resultar útiles en tareas posteriores, como clasificación o análisis de datos.e datos.e datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9febf16c-a500-4954-ad17-7acc4755e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11348/11348 [==============================] - 41s 3ms/step - loss: 39112.4141\n",
      "Epoch 2/50\n",
      "11348/11348 [==============================] - 40s 3ms/step - loss: 27138.8789\n",
      "Epoch 3/50\n",
      "11348/11348 [==============================] - 60s 5ms/step - loss: 18202.5039\n",
      "Epoch 4/50\n",
      "11348/11348 [==============================] - 39s 3ms/step - loss: 11968.6846\n",
      "Epoch 5/50\n",
      "11348/11348 [==============================] - 46s 4ms/step - loss: 7783.1396\n",
      "Epoch 6/50\n",
      "11348/11348 [==============================] - 40s 4ms/step - loss: 5136.2964\n",
      "Epoch 7/50\n",
      "11348/11348 [==============================] - 37s 3ms/step - loss: 7856.4814\n",
      "Epoch 8/50\n",
      "11348/11348 [==============================] - 52s 5ms/step - loss: 5401.4805\n",
      "Epoch 9/50\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 4266.5596\n",
      "Epoch 10/50\n",
      "11348/11348 [==============================] - 41s 4ms/step - loss: 4352.4668\n",
      "Epoch 11/50\n",
      "11348/11348 [==============================] - 40s 4ms/step - loss: 6965.8164\n",
      "Epoch 12/50\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 6103.7168\n",
      "Epoch 13/50\n",
      "11348/11348 [==============================] - 35s 3ms/step - loss: 3796.4792\n",
      "Epoch 14/50\n",
      "11348/11348 [==============================] - 40s 4ms/step - loss: 2772.0605\n",
      "Epoch 15/50\n",
      "11348/11348 [==============================] - 43s 4ms/step - loss: 2759.2295\n",
      "Epoch 16/50\n",
      "11348/11348 [==============================] - 31s 3ms/step - loss: 2757.4365\n",
      "Epoch 17/50\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 2757.0088\n",
      "Epoch 18/50\n",
      "11348/11348 [==============================] - 37s 3ms/step - loss: 2757.4182\n",
      "Epoch 19/50\n",
      "11348/11348 [==============================] - 33s 3ms/step - loss: 2757.1951\n",
      "Epoch 20/50\n",
      "11348/11348 [==============================] - 34s 3ms/step - loss: 2756.8118\n",
      "Epoch 21/50\n",
      "11348/11348 [==============================] - 40s 3ms/step - loss: 2756.9050\n",
      "Epoch 22/50\n",
      "11348/11348 [==============================] - 36s 3ms/step - loss: 2756.9099\n",
      "Epoch 23/50\n",
      "11348/11348 [==============================] - 60s 5ms/step - loss: 2757.1865\n",
      "Epoch 24/50\n",
      "11348/11348 [==============================] - 40s 3ms/step - loss: 2756.9285\n",
      "Epoch 25/50\n",
      "11348/11348 [==============================] - 50s 4ms/step - loss: 2751.7332\n",
      "Epoch 26/50\n",
      "11348/11348 [==============================] - 90s 8ms/step - loss: 2748.2178\n",
      "Epoch 27/50\n",
      "11348/11348 [==============================] - 47s 4ms/step - loss: 2747.8464\n",
      "Epoch 28/50\n",
      "11348/11348 [==============================] - 52s 5ms/step - loss: 2747.3657\n",
      "Epoch 29/50\n",
      "11348/11348 [==============================] - 46s 4ms/step - loss: 2747.2500\n",
      "Epoch 30/50\n",
      "11348/11348 [==============================] - 40s 3ms/step - loss: 2747.1587\n",
      "Epoch 31/50\n",
      "11348/11348 [==============================] - 44s 4ms/step - loss: 2601.2671\n",
      "Epoch 32/50\n",
      "11348/11348 [==============================] - 51s 4ms/step - loss: 2089.9314\n",
      "Epoch 33/50\n",
      "11348/11348 [==============================] - 45s 4ms/step - loss: 2065.0342\n",
      "Epoch 34/50\n",
      "11348/11348 [==============================] - 44s 4ms/step - loss: 2044.5276\n",
      "Epoch 35/50\n",
      "11348/11348 [==============================] - 41s 4ms/step - loss: 2027.0096\n",
      "Epoch 36/50\n",
      "11348/11348 [==============================] - 47s 4ms/step - loss: 2011.4385\n",
      "Epoch 37/50\n",
      "11348/11348 [==============================] - 48s 4ms/step - loss: 1998.2454\n",
      "Epoch 38/50\n",
      "11348/11348 [==============================] - 52s 5ms/step - loss: 1987.0289\n",
      "Epoch 39/50\n",
      "11348/11348 [==============================] - 52s 5ms/step - loss: 1976.6814\n",
      "Epoch 40/50\n",
      "11348/11348 [==============================] - 49s 4ms/step - loss: 1966.9437\n",
      "Epoch 41/50\n",
      "11348/11348 [==============================] - 46s 4ms/step - loss: 1958.8723\n",
      "Epoch 42/50\n",
      "11348/11348 [==============================] - 43s 4ms/step - loss: 1952.6478\n",
      "Epoch 43/50\n",
      "11348/11348 [==============================] - 48s 4ms/step - loss: 1947.4187\n",
      "Epoch 44/50\n",
      "11348/11348 [==============================] - 49s 4ms/step - loss: 1942.5317\n",
      "Epoch 45/50\n",
      "11348/11348 [==============================] - 48s 4ms/step - loss: 1937.6265\n",
      "Epoch 46/50\n",
      "11348/11348 [==============================] - 47s 4ms/step - loss: 1744.6813\n",
      "Epoch 47/50\n",
      "11348/11348 [==============================] - 57s 5ms/step - loss: 1255.7168\n",
      "Epoch 48/50\n",
      "11348/11348 [==============================] - 51s 5ms/step - loss: 1237.1086\n",
      "Epoch 49/50\n",
      "11348/11348 [==============================] - 43s 4ms/step - loss: 1219.5701\n",
      "Epoch 50/50\n",
      "11348/11348 [==============================] - 43s 4ms/step - loss: 1216.2814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28369b2af90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el autoencoder\n",
    "autoencoder.fit(features_array, features_array, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3ed72-6b17-4d40-be83-3f9d57b364fb",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "Este código implementa un autoencoder utilizando TensorFlow y Keras para realizar la reducción de dimensionalidad de un conjunto de datos. En primer lugar, se seleccionan las columnas de características del DataFrame `df`, excluyendo la columna \"etiqueta\", que generalmente se utiliza para etiquetar los datos y no como parte de las características. Luego, se utiliza `VectorAssembler` de PySpark para ensamblar las características seleccionadas en una única columna llamada \"features\", lo que facilita la preparación de los datos para el autoencoder. Posteriormente, se convierte el DataFrame resultante de PySpark en un array NumPy llamado `features_array` para poder trabajar con los datos en TensorFlow.\n",
    "\n",
    "La arquitectura del autoencoder se define en las siguientes líneas, con una capa de entrada que tiene la misma dimensión que el número de características de entrada y una capa de salida que también tiene la misma dimensión, permitiendo la reconstrucción de los datos originales. En la capa de codificación se utiliza la función de activación 'sigmoid', mientras que en la capa de decodificación se utiliza 'relu'. Además, se aplica una regularización L2 en la capa de codificación para evitar el sobreajuste.\n",
    "\n",
    "El modelo del autoencoder se compila utilizando el optimizador Adam con una tasa de aprendizaje adaptativa y la función de pérdida de error cuadrático medio (MSE), que mide la discrepancia entre los datos reales y los datos reconstruidos. \n",
    "\n",
    "La parte más importante es el entrenamiento del autoencoder. Durante 50 épocas y con un tamaño de lote (batch size) de 32, el modelo aprende a representar eficazmente los datos de entrada en un espacio de menor dimensionalidad y a reconstruirlos con la menor pérdida de información posible. La pérdida (loss) se muestra en cada época para evaluar qué tan bien el autoencoder puede reconstruir los datos originales después de la reducción de dimensionalidad.\n",
    "\n",
    "En cuanto a los cambios en la pérdida (loss) durante el entrenamiento, estos reflejan cómo el autoencoder está mejorando en su capacidad para representar y reconstruir los datos. Inicialmente, la pérdida puede ser alta, pero a medida que el modelo se ajusta a los datos, la pérdida tiende a disminuir. La interpretación es que el autoencoder está aprendiendo a capturar las características importantes de los datos en su representación comprimida y a utilizar esa representación para una reconstrucción más precisa. En este caso, vemos una disminución de la pérdida en las primeras épocas, lo que sugiere que el autoencoder está mejorando en su tarea de reducción de dimensionalidad y reconstrucción de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0de07d7-9e07-4d39-8ce2-191d6a599dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11348/11348 [==============================] - 39s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtener las representaciones codificadas de los datos\n",
    "encoded_features_model = tf.keras.models.Model(inputs=input_layer, outputs=encoder)\n",
    "encoded_features = encoded_features_model.predict(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b071f959-07dc-414b-95a3-18d52df856f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las representaciones codificadas de vuelta a un DataFrame de Spark\n",
    "encoded_features_rdd = spark.sparkContext.parallelize(encoded_features.tolist())\n",
    "encoded_features_df = encoded_features_rdd.map(lambda x: (Vectors.dense(x),)).toDF([\"encoded_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53666c2-8e1c-42b4-a6e5-ed38cb3ce295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5674/5674 [==============================] - 24s 4ms/step - loss: 1214.4604\n",
      "Epoch 2/50\n",
      "5674/5674 [==============================] - 41s 7ms/step - loss: 1213.5175\n",
      "Epoch 3/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1212.7852\n",
      "Epoch 4/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1212.1895\n",
      "Epoch 5/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1211.6528\n",
      "Epoch 6/50\n",
      "5674/5674 [==============================] - 30s 5ms/step - loss: 1211.1935\n",
      "Epoch 7/50\n",
      "5674/5674 [==============================] - 33s 6ms/step - loss: 1210.7482\n",
      "Epoch 8/50\n",
      "5674/5674 [==============================] - 24s 4ms/step - loss: 1210.3453\n",
      "Epoch 9/50\n",
      "5674/5674 [==============================] - 25s 4ms/step - loss: 1209.9443\n",
      "Epoch 10/50\n",
      "5674/5674 [==============================] - 33s 6ms/step - loss: 1209.6063\n",
      "Epoch 11/50\n",
      "5674/5674 [==============================] - 31s 5ms/step - loss: 1209.2941\n",
      "Epoch 12/50\n",
      "5674/5674 [==============================] - 23s 4ms/step - loss: 1209.0273\n",
      "Epoch 13/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1208.7947\n",
      "Epoch 14/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1208.5741\n",
      "Epoch 15/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1208.3856\n",
      "Epoch 16/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1208.2069\n",
      "Epoch 17/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1208.0331\n",
      "Epoch 18/50\n",
      "5674/5674 [==============================] - 21s 4ms/step - loss: 1207.8741\n",
      "Epoch 19/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1207.7567\n",
      "Epoch 20/50\n",
      "5674/5674 [==============================] - 30s 5ms/step - loss: 1207.6396\n",
      "Epoch 21/50\n",
      "5674/5674 [==============================] - 33s 6ms/step - loss: 1207.5367\n",
      "Epoch 22/50\n",
      "5674/5674 [==============================] - 33s 6ms/step - loss: 1207.4453\n",
      "Epoch 23/50\n",
      "5674/5674 [==============================] - 27s 5ms/step - loss: 1207.3461\n",
      "Epoch 24/50\n",
      "5674/5674 [==============================] - 34s 6ms/step - loss: 1207.2578\n",
      "Epoch 25/50\n",
      "5674/5674 [==============================] - 28s 5ms/step - loss: 1207.1742\n",
      "Epoch 26/50\n",
      "5674/5674 [==============================] - 27s 5ms/step - loss: 1207.0819\n",
      "Epoch 27/50\n",
      "5674/5674 [==============================] - 36s 6ms/step - loss: 1207.0044\n",
      "Epoch 28/50\n",
      "5674/5674 [==============================] - 28s 5ms/step - loss: 1206.9166\n",
      "Epoch 29/50\n",
      "5674/5674 [==============================] - 25s 4ms/step - loss: 1206.8381\n",
      "Epoch 30/50\n",
      "5674/5674 [==============================] - 35s 6ms/step - loss: 1206.7601\n",
      "Epoch 31/50\n",
      "5674/5674 [==============================] - 28s 5ms/step - loss: 1206.6808\n",
      "Epoch 32/50\n",
      "5674/5674 [==============================] - 29s 5ms/step - loss: 1206.6007\n",
      "Epoch 33/50\n",
      "5674/5674 [==============================] - 32s 6ms/step - loss: 1206.5265\n",
      "Epoch 34/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1206.4512\n",
      "Epoch 35/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1206.3802\n",
      "Epoch 36/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1206.3114\n",
      "Epoch 37/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1206.2543\n",
      "Epoch 38/50\n",
      "5674/5674 [==============================] - 26s 5ms/step - loss: 1206.2117\n",
      "Epoch 39/50\n",
      "5674/5674 [==============================] - 29s 5ms/step - loss: 1206.1849\n",
      "Epoch 40/50\n",
      "5674/5674 [==============================] - 21s 4ms/step - loss: 1206.1737\n",
      "Epoch 41/50\n",
      "5674/5674 [==============================] - 22s 4ms/step - loss: 1206.1654\n",
      "Epoch 42/50\n",
      "4199/5674 [=====================>........] - ETA: 6s - loss: 1212.1324"
     ]
    }
   ],
   "source": [
    "# Entrenar el autoencoder\n",
    "history = autoencoder.fit(features_array, features_array, epochs=50, batch_size=64)\n",
    "\n",
    "# Imprimir métricas durante el entrenamiento\n",
    "print(\"Métricas durante el entrenamiento:\")\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dcb28-5c85-43b2-8f8c-7a985d11a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la pérdida durante el entrenamiento\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663c554-d04c-47f9-9020-9f9d479d94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Spark a un array NumPy\n",
    "encoded_features_array = np.array(encoded_features_df.select(\"encoded_features\").rdd.map(lambda x: x.encoded_features.toArray()).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23ee17-66f7-4b6d-b50d-17e691a06ba1",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "Primero, se crea un nuevo modelo llamado `encoded_features_model`, que se basa en el modelo original del autoencoder. Sin embargo, este nuevo modelo está diseñado para tomar las mismas entradas que el modelo original, pero produce las salidas de la capa de codificación. Es decir, toma los datos de entrada y genera las representaciones codificadas de esos datos. Estas representaciones codificadas se almacenan en la variable `encoded_features`.\n",
    "\n",
    "A continuación, las representaciones codificadas se convierten en un formato adecuado para trabajar con Spark. Inicialmente, están en forma de una estructura de datos NumPy, pero se convierten en un RDD (Resilient Distributed Dataset) de Spark utilizando `spark.sparkContext.parallelize`. Luego, se mapean para crear un DataFrame de Spark llamado `encoded_features_df`. Esto es útil para integrar las representaciones codificadas en el entorno de Spark, lo que permite realizar análisis posteriores o procesamientos de datos con Spark.\n",
    "\n",
    "El código también realiza un entrenamiento adicional del autoencoder durante 50 épocas con un tamaño de lote de 64. Aunque el autoencoder ya se entrenó previamente, este paso podría ser útil para afinar aún más el modelo o verificar su estabilidad después de obtener las representaciones codificadas.\n",
    "\n",
    "Luego, se imprime un resumen de las métricas de entrenamiento, que generalmente incluyen información sobre la pérdida (loss) en cada época. Esto proporciona una visión general de cómo el modelo está convergiendo y mejorando su capacidad para representar los datos de manera eficiente.\n",
    "\n",
    "Además, se crea un gráfico utilizando la biblioteca Matplotlib que visualiza cómo cambió la pérdida durante las épocas de entrenamiento. Este gráfico ayuda a evaluar la convergencia del modelo y su capacidad para reducir la pérdida a medida que se ajusta a los datos.\n",
    "\n",
    "Finalmente, las representaciones codificadas se extraen del DataFrame de Spark y se convierten nuevamente en un array NumPy llamado `encoded_features_array`. Esto puede ser útil si se necesita utilizar las representaciones codificadas en operaciones posteriores que requieran NumPy en lugar de Spark. En resumen, este código facilita la obtención y el manejo de las representaciones codificadas generadas por el autoencoder después del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9339f6-c3ee-4930-94a7-380119b229e4",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "# KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3d9e6-e0e0-4244-b2c0-1cd49f9b09ff",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "  \n",
    "K-Means es un algoritmo de aprendizaje automático no supervisado utilizado en análisis de datos y minería de datos para agrupar datos en clusters o grupos basados en similitudes. El objetivo principal del algoritmo K-Means es dividir un conjunto de datos en K grupos, donde K es un número predefinido de clusters que el usuario especifica antes de ejecutar el algoritmo. Cada cluster contiene puntos de datos que son más similares entre sí que con los puntos en otros clusters.\r\n",
    "\r\n",
    "El algoritmo K-Means funciona de la siguiente manera:\r\n",
    "\r\n",
    "1. Inicialización: Se seleccionan aleatoriamente K puntos de datos como centroides iniciales. Estos centroides representan el centro de cada cluster.\r\n",
    "\r\n",
    "2. Asignación de puntos a clusters: Para cada punto de datos en el conjunto de datos, se calcula la distancia entre el punto y cada uno de los centroides. El punto se asigna al cluster cuyo centroide está más cerca de él.\r\n",
    "\r\n",
    "3. Actualización de centroides: Una vez que todos los puntos han sido asignados a clusters, se recalcula el centroide de cada cluster tomando la media de todos los puntos asignados a ese cluster.\r\n",
    "\r\n",
    "4. Repetición de pasos 2 y 3: Los pasos 2 y 3 se repiten iterativamente hasta que los centroides de los clusters ya no cambien significativamente o se alcance un número máximo de iteraciones predefinido.\r\n",
    "\r\n",
    "El resultado final del algoritmo K-Means es un conjunto de K clusters, cada uno con un centroide que representa el punto central del cluster. Este algoritmo es útil para diversas aplicaciones, como segmentación de clientes, comprensión de patrones en datos y reducción de dimensionalidad.\r\n",
    "\r\n",
    "Es importante destacar que K-Means es sensible a la inicialización de los centroides, lo que significa que diferentes selecciones iniciales de centroides pueden conducir a soluciones de agrupamiento diferentes. Por lo tanto, en la práctica, a menudo se realizan múltiples ejecuciones del algoritmo con diferentes inicializaciones y se selecciona la mejor solución en función de algún criterio, como la suma de las distancias al cuadrado dentro del cluster (inertia) o la silueta de los clusters. los clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b08ff6-a3f1-4943-bd60-51a03d094b57",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "## Criterio del codo\n",
    "\n",
    "El criterio del codo es una técnica comúnmente utilizada en análisis de clusters (clustering) para ayudar a determinar el número óptimo de clusters en un conjunto de datos. Su objetivo es identificar el punto en el que el incremento en la varianza explicada por los clusters adicionales se vuelve significativamente menor, lo que se asemeja a un \"codo\" en un gráfico. Este método es útil para tomar decisiones informadas sobre el número apropiado de clusters a utilizar en un algoritmo de agrupamiento, como K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb53d6-980d-47a3-8913-42bcaca11203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista vacía para almacenar las inercias\n",
    "cs = []\n",
    "\n",
    "# Probar diferentes valores de k (número de clusters)\n",
    "for i in range(1, 20):\n",
    "    kmeans = SKLearnKMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(encoded_features_array)  # Usar array Numpy de características codificadas\n",
    "\n",
    "    # Calcular la inercia y añadirla a la lista\n",
    "    cs.append(kmeans.inertia_)\n",
    "\n",
    "# Trazar la curva de la inercia en función del número de clusters\n",
    "plt.plot(range(1, 20), cs, marker='o', linestyle='-', color='blue')\n",
    "plt.xlabel('Número de Clusters (k)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Criterio del Codo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8ab0c-8da1-4f81-be45-bc1585d453aa",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "## Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d3d7a-08fd-4f41-a771-4589a60fbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que las características estén en formato de vector\n",
    "assembler = VectorAssembler(inputCols=[\"encoded_features\"], outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(encoded_features_df)\n",
    "\n",
    "# Estandarizar las características\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(vectorized_df)\n",
    "scaled_df = scalerModel.transform(vectorized_df)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pcaFeatures\")\n",
    "pcaModel = pca.fit(scaled_df)\n",
    "result = pcaModel.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00221ab7-537b-4b7c-9969-fdf0b60cc0ce",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "En primer lugar, se utiliza un paso llamado `VectorAssembler` para asegurarse de que las características de los datos estén en un formato adecuado. Esto es importante porque muchas operaciones y modelos en PySpark requieren que las características se encuentren en un solo vector. El código utiliza `VectorAssembler` para combinar las columnas de entrada especificadas en un único vector llamado \"features\". El resultado de este paso se almacena en un nuevo DataFrame llamado `vectorized_df`.\n",
    "\n",
    "A continuación, se aplica un proceso de estandarización a las características utilizando `StandardScaler`. La estandarización implica ajustar las características de manera que tengan una media de 0 y una desviación estándar de 1. Esto es útil para asegurarse de que todas las características estén en la misma escala y facilitar el procesamiento posterior. En este caso, `withStd=True` indica que se debe aplicar la desviación estándar, mientras que `withMean=False` indica que no se debe centrar las características en cero. El resultado se almacena en un DataFrame llamado `scaled_df`.\n",
    "\n",
    "Después de la estandarización, se lleva a cabo el análisis de componentes principales (PCA). PCA es una técnica de reducción de dimensionalidad que busca encontrar las dimensiones más importantes o \"componentes principales\" en los datos. En este caso, el código configura PCA para retener únicamente las dos primeras componentes principales (`k=2`). Las características estandarizadas se utilizan como entrada, y el resultado se almacena en una nueva columna llamada \"pcaFeatures\" en el DataFrame `result`.\n",
    "\n",
    "Finalmente, se aplica el modelo PCA al DataFrame estandarizado `scaled_df`, y el resultado se almacena en un nuevo DataFrame llamado `result`. Este último DataFrame contiene las características reducidas por PCA en la columna \"pcaFeatures\". Estas características representan la información más relevante y compacta de los datos originales en un espacio de menor dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c3c0b-bb86-46c4-8810-24e639019797",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "## Entrenamiento del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c7c92-a985-48c8-a1e6-7aee1db53eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el objeto KMeans\n",
    "kmeans = KMeans(k=2, featuresCol=\"pcaFeatures\", predictionCol=\"cluster\")\n",
    "\n",
    "# Entrenar el modelo KMeans\n",
    "kmeans_model = kmeans.fit(result)\n",
    "\n",
    "# Obtener las predicciones\n",
    "predictions = kmeans_model.transform(result)\n",
    "\n",
    "# Mostrar los resultados\n",
    "predictions.select('pcaFeatures', 'cluster').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db910f1-47b5-4f57-b6ca-a36107012d80",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "\n",
    "Dado un conjunto de datos $X = \\{x_1, x_2, \\ldots, x_n\\}$ donde $x_i$ representa un punto de datos en un espacio $d$-dimensional, y deseamos dividirlos en $K$ clusters:\n",
    "\n",
    "1\\. Inicialización de centroides: Seleccionamos aleatoriamente $K$ centroides iniciales $\\{\\mu_1, \\mu_2, \\ldots, \\mu_K\\}$.\n",
    "\n",
    "2\\. Asignación de puntos a clusters: Para cada punto $x_i$, calculamos la distancia euclidiana entre $x_i$ y cada centroide $\\mu_j$:\n",
    "\n",
    "$$d(x_i, \\mu_j) = \\sqrt{\\sum_{k=1}^{d}(x_{ik} - \\mu_{jk})^2}$$\n",
    "\n",
    "Asignamos $x_i$ al cluster cuyo centroide tiene la distancia más pequeña:\n",
    "\n",
    "$$c_i = \\arg\\min_{j} d(x_i, \\mu_j)$$\n",
    "\n",
    "Donde $c_i$ representa el índice del cluster al que se asigna $x_i$.\n",
    "\n",
    "3\\. Actualización de centroides: Calculamos nuevos centroides para cada cluster tomando la media de todos los puntos asignados a ese cluster:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i$$\n",
    "\n",
    "Donde $|C_j|$ representa el número de puntos en el cluster $j$.\n",
    "\n",
    "4\\. Repetición de pasos 2 y 3: Los pasos 2 y 3 se repiten iterativamente hasta que se cumple un criterio de detención, como cuando los centroides ya no cambian significativamente o se alcanza un número máximo iteraciones. \n",
    "\n",
    "El objetivo del algoritmo K-Means es minimizar la función de costo, que se define como la suma de las distancias al cuadrado dentro de los clusters:\n",
    "\n",
    "$$J = \\sum_{j=1}^{K} \\sum_{i \\in C_j} d(x_i, \\mu_j)^2$$\n",
    "\n",
    "En resumen, el K-Means busca encontrar $K$ centroides que minimicen esta función de costo ajustando los centroides y asignando puntos de datos a clusters de manera iterativa. El resultado final es un conjunto de clusters donde cada punto de datos está asignado al cluster cuyo centroide es el más cercano en términos de distancia euclidiana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb15457-d7ba-473a-90a5-64172fd09715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficiente de silueta\n",
    "evaluator = ClusteringEvaluator(predictionCol=\"cluster\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Coeficiente de silueta:\", silhouette)\n",
    "\n",
    "# Puntuación de Calinski-Harabasz\n",
    "calinski_harabasz = evaluator.evaluate(predictions, {evaluator.metricName: \"silhouette\"})\n",
    "print(\"Puntuación de Calinski-Harabasz:\", calinski_harabasz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02305a02-928c-40ec-84aa-9d45294afb46",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Garamond'; font-size: 14px;\">\n",
    "    \n",
    "Primero, el \"Coeficiente de Silueta\" (Silhouette Score) es una métrica que evalúa la cohesión y separación de los clusters obtenidos. Esta métrica asigna a cada punto de datos un valor que indica cuán similar es ese punto a los demás puntos en su mismo cluster en comparación con los puntos en clusters vecinos. Los valores del coeficiente de silueta oscilan entre -1 y 1, donde un valor alto (cerca de 1) indica una buena calidad de clustering con clusters bien definidos y separados, mientras que un valor bajo (cerca de -1) sugiere que los clusters se superponen o están mal definidos. En el código, se utiliza el evaluador `ClusteringEvaluator` para calcular el coeficiente de silueta a partir de las predicciones del modelo de clustering, y el resultado se almacena en la variable `silhouette`.\n",
    "\n",
    "Por otro lado, la \"Puntuación de Calinski-Harabasz\" (Calinski-Harabasz Score) es otra métrica que evalúa la calidad de los clusters formados por un algoritmo de clustering. Esta métrica mide la relación entre la dispersión interna de los clusters y la dispersión entre clusters. Un valor alto de la puntuación de Calinski-Harabasz indica que los clusters son compactos y bien separados. En el código, se configura el evaluador `ClusteringEvaluator` con el parámetro `metricName` establecido en \"silhouette\" para calcular la puntuación de Calinski-Harabasz en lugar del coeficiente de silueta, y el resultado se almacena en la variable `calinski_harabasz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d615b90-c41f-44ac-9d48-10f064ae968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir Spark DataFrame a Pandas DataFrame para visualización\n",
    "predictions_pd = predictions.toPandas()\n",
    "\n",
    "pcaFeatures_array = np.array(predictions_pd['pcaFeatures'].tolist())\n",
    "# Extraer componentes de 'pcaFeatures' en nuevas columnas\n",
    "predictions_pd['pca_x'] = pcaFeatures_array[:, 0]\n",
    "predictions_pd['pca_y'] = pcaFeatures_array[:, 1]\n",
    "\n",
    "# Visualización 2D de los clusters utilizando Seaborn y la paleta \"mako\"\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(data=predictions_pd, x='pca_x', y='pca_y', hue='cluster', palette='mako', s=50, legend='full')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('2D Visualization of Clusters with Mako Palette')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
